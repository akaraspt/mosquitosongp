{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9654,
     "status": "ok",
     "timestamp": 1610459538556,
     "user": {
      "displayName": "BORVORNTAT NIRANDMONGKOL",
      "photoUrl": "",
      "userId": "13784846662955035094"
     },
     "user_tz": -420
    },
    "id": "Vp7RBG7LXdIA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import random\n",
    "import librosa\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from soundfile import write as wav_write\n",
    "# from pydub import AudioSegment\n",
    "import array\n",
    "plt.style.use('dark_background')\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "configproto = tf.compat.v1.ConfigProto() \n",
    "configproto.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=configproto) \n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9654,
     "status": "ok",
     "timestamp": 1610459538553,
     "user": {
      "displayName": "BORVORNTAT NIRANDMONGKOL",
      "photoUrl": "",
      "userId": "13784846662955035094"
     },
     "user_tz": -420
    },
    "id": "MmbH-y6OWgdQ",
    "outputId": "1194e06e-0ee6-4117-ee27-62369c2d05ba"
   },
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name != '/device:GPU:0':\n",
    "        print(\"NO GPU\")\n",
    "    print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92582,
     "status": "ok",
     "timestamp": 1610459621497,
     "user": {
      "displayName": "BORVORNTAT NIRANDMONGKOL",
      "photoUrl": "",
      "userId": "13784846662955035094"
     },
     "user_tz": -420
    },
    "id": "Y7oO8NQOXeKG",
    "outputId": "c9fd2e1b-f1db-4b1c-935d-f75104c94d56"
   },
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    audio_files_name = os.listdir(directory)\n",
    "    sorted_audio_files_name = sorted(audio_files_name, key=numericalSort)\n",
    "    return sorted_audio_files_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113332,
     "status": "ok",
     "timestamp": 1610459642331,
     "user": {
      "displayName": "BORVORNTAT NIRANDMONGKOL",
      "photoUrl": "",
      "userId": "13784846662955035094"
     },
     "user_tz": -420
    },
    "id": "EcOeiIsV27uO",
    "outputId": "5763911d-3c51-4e83-f184-56be267d2eb6"
   },
   "outputs": [],
   "source": [
    "def encode_data(dataY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    Y = le.fit_transform(dataY)\n",
    "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    return Y, le_name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113324,
     "status": "ok",
     "timestamp": 1610459642333,
     "user": {
      "displayName": "BORVORNTAT NIRANDMONGKOL",
      "photoUrl": "",
      "userId": "13784846662955035094"
     },
     "user_tz": -420
    },
    "id": "U6D07X-k5Zrg",
    "outputId": "5f829075-cdaf-4a3f-af8b-656590d2d908"
   },
   "outputs": [],
   "source": [
    "def split_train_test_val(dataX, Y):    \n",
    "    X_train, X_test_val, y_train, y_test_val= train_test_split(dataX, Y, test_size=0.2, random_state=42, shuffle=True, stratify=Y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42, shuffle=True, stratify=y_test_val)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114309,
     "status": "ok",
     "timestamp": 1610459643327,
     "user": {
      "displayName": "BORVORNTAT NIRANDMONGKOL",
      "photoUrl": "",
      "userId": "13784846662955035094"
     },
     "user_tz": -420
    },
    "id": "j0rGn2Ipq0EH",
    "outputId": "d6d67988-d0be-43de-a4ca-e06c02c82e01"
   },
   "outputs": [],
   "source": [
    "def balance_train(X_train, y_train):\n",
    "    sm = RandomOverSampler(sampling_strategy='all', random_state=42)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 114309,
     "status": "ok",
     "timestamp": 1610459643332,
     "user": {
      "displayName": "BORVORNTAT NIRANDMONGKOL",
      "photoUrl": "",
      "userId": "13784846662955035094"
     },
     "user_tz": -420
    },
    "id": "3Kb79qjL5inA"
   },
   "outputs": [],
   "source": [
    "def reshape_data(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    X_train = tf.convert_to_tensor(X_train)\n",
    "    X_test = tf.convert_to_tensor(X_test)\n",
    "    X_val = tf.convert_to_tensor(X_val)\n",
    "    y_train = tf.convert_to_tensor(y_train)\n",
    "    y_test = tf.convert_to_tensor(y_test)\n",
    "    y_val = tf.convert_to_tensor(y_val)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_val = to_categorical(y_val)\n",
    "    y_test = to_categorical(y_test)\n",
    "    X_train = tf.reshape(X_train ,(X_train.shape[0], X_train.shape[1], -1))\n",
    "    X_test = tf.reshape(X_test ,(X_test.shape[0], X_test.shape[1], -1))\n",
    "    X_val = tf.reshape(X_val ,(X_val.shape[0], X_val.shape[1], -1))\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiplyFactorToSound(sound_array, factor):\n",
    "    factored_array = np.multiply(sound_array, factor) # multiply with factor\n",
    "    return factored_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delay(M1, M2,sr, x ,y):\n",
    "    c = 343 # speed of sound in the air = 343 m/s\n",
    "    xM1_dist = math.hypot(M1[0] - x, M1[1] - y)\n",
    "    xM2_dist = math.hypot(M2[0] - x, M2[1] - y)\n",
    "    \n",
    "    \n",
    "    xM1_delay_duration = abs(xM1_dist)/ c\n",
    "    xM2_delay_duration = abs(xM2_dist)/ c\n",
    "    \n",
    "    xM1_delay = np.zeros(int(xM1_delay_duration * sr))\n",
    "    xM2_delay = np.zeros(int(xM2_delay_duration * sr))\n",
    "    return xM1_dist, xM2_dist, xM1_delay,xM2_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_noise_channels(noise_in_two_channels):\n",
    "    return noise_in_two_channels[1],noise_in_two_channels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shifted_noise(noise_1, noise_2, len_mosquito):\n",
    "    assert len(noise_1) == len(noise_2), \"both noise channel must have the same length\"\n",
    "    randomable_range = 0\n",
    "    endpoint = 0\n",
    "    randomable_range = len(noise_1) - len_mosquito\n",
    "    random_start = int(round(random.uniform(0, randomable_range), 0))\n",
    "    \n",
    "    end = random_start + len_mosquito\n",
    "    random_shifted_noise_1 = noise_1[random_start: end]\n",
    "    random_shifted_noise_2 = noise_2[random_start: end]\n",
    "\n",
    "    return random_shifted_noise_1, random_shifted_noise_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shifted_noise_with_seed(noise_1, noise_2, len_mosquito, seed):\n",
    "    assert len(noise_1) == len(noise_2), \"both noise channel must have the same length\"\n",
    "    randomable_range = 0\n",
    "    endpoint = 0\n",
    "    randomable_range = len(noise_1) - len_mosquito\n",
    "    random.seed(seed)\n",
    "    random_start = int(round(random.uniform(0, randomable_range), 0))\n",
    "    \n",
    "    end = random_start + len_mosquito\n",
    "    random_shifted_noise_1 = noise_1[random_start: end]\n",
    "    random_shifted_noise_2 = noise_2[random_start: end]\n",
    "\n",
    "    return random_shifted_noise_1, random_shifted_noise_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dir = \"./data/environmental_noise_recordings/\"\n",
    "\n",
    "noise_list = os.listdir(noise_dir)\n",
    "noise_list.sort()\n",
    "noise_sound = []\n",
    "for idx, filename in enumerate(noise_list):\n",
    "    noise_two_channels, sr = librosa.load(noise_dir + filename, mono = False, sr =8000)\n",
    "    print(filename)\n",
    "    noise_sound.append(noise_two_channels)\n",
    "print(f\"Amount of noise files: {len(noise_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modification of splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_mosquito_filename_without_noise(species,sex, day, lowMic):\n",
    "    species_sex = (species + '_' + sex).strip()\n",
    "    day = day.strip()\n",
    "    isLowMic = lowMic.strip() == \"LowMic\"\n",
    "    if isLowMic:\n",
    "        key = species_sex + '_' + day + '_' + \"LowMic\"\n",
    "    else:\n",
    "        key = species_sex + '_' + day \n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_species_dict_without_noise(sorted_audio_files_name):\n",
    "#     np_files = np.array(sorted_audio_files_name)\n",
    "    keySet = set()\n",
    "    for file in sorted_audio_files_name:\n",
    "        row = file.split('_')\n",
    "        key = recreate_mosquito_filename_without_noise(row[0], row[1], row[2], row[5])\n",
    "        keySet.add(key)\n",
    "    keyList = list(keySet)\n",
    "    empty_species_dict = dict.fromkeys(keyList, 0)\n",
    "    return empty_species_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_data(sorted_audio_files_name, dir='./SmallCylinder_big/'):\n",
    "    count_species = create_species_dict_without_noise(sorted_audio_files_name)\n",
    "    np.random.seed(42)\n",
    "    sr = 8000\n",
    "    target = 2400  # 0.3s\n",
    "    count = 0\n",
    "    for p in sorted_audio_files_name:\n",
    "        path = dir + p\n",
    "        if path.endswith('.wav'):\n",
    "            row = p.split('_')\n",
    "            x, sr = librosa.load(path, sr=sr)\n",
    "            for i in range(0, len(x), int(target / 2)):  #overlap .15s\n",
    "                np.random.seed(i)\n",
    "                shift = np.random.randint(120)\n",
    "                y = x[i + shift : i + target + shift]\n",
    "                if (len(y) == target):\n",
    "                    key = recreate_mosquito_filename_without_noise(row[0], row[1], row[2], row[5])\n",
    "                    count = count + 1\n",
    "                    count_species[key] = count_species[key] + 1\n",
    "    return count_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_test_low =  {'Cx.Quin_1F_14D_LowMic': 97,\n",
    "                   'Ae.Aegypti_1F_21D_LowMic': 19, \n",
    "                   'An.Dirus_1M_14D_LowMic': 30,\n",
    "                   'An.Dirus_1F_14D_LowMic': 34,\n",
    "                   'Ae.Aegypti_1M_21D_LowMic': 39,\n",
    "                   'Ae.Albopictus_1F_7D_LowMic': 51,\n",
    "                   'Ae.Albopictus_1M_7D_LowMic': 46,\n",
    "                   'Cx.Quin_1M_14D_LowMic': 72,\n",
    "                   'Ae.Aegypti_1F_5D_LowMic': 64, \n",
    "                   'Ae.Albopictus_1F_14D_LowMic': 43,\n",
    "                   'Ae.Albopictus_1M_14D_LowMic': 57,\n",
    "                   'Ae.Aegypti_1M_5D_LowMic': 48}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(sorted_audio_files_name, total_data, fold_round, dir):\n",
    "    count_species = create_species_dict_without_noise(sorted_audio_files_name)\n",
    "    count_training_set = create_species_dict_without_noise(sorted_audio_files_name)\n",
    "    test_index = -1 if fold_round == 9 else fold_round\n",
    "    low_count = {'Cx.Quin_1F_14D_LowMic': 0,\n",
    "                   'Ae.Aegypti_1F_21D_LowMic': 0, \n",
    "                   'An.Dirus_1F_5D_LowMic': 0,\n",
    "                   'An.Dirus_1M_5D_LowMic': 0,\n",
    "                   'An.Dirus_1M_14D_LowMic': 0,\n",
    "                   'An.Minimus_1F_5D_LowMic': 0,\n",
    "                   'An.Minimus_1M_5D_LowMic': 0,\n",
    "                   'An.Dirus_1F_14D_LowMic': 0,\n",
    "                   'Ae.Aegypti_1M_21D_LowMic': 0,\n",
    "                   'Cx.Quin_1M_6D_LowMic': 0,\n",
    "                   'Ae.Albopictus_1F_7D_LowMic': 0,\n",
    "                   'Ae.Albopictus_1M_7D_LowMic': 0,\n",
    "                   'Cx.Quin_1F_6D_LowMic': 0,\n",
    "                   'Cx.Quin_1M_14D_LowMic': 0,\n",
    "                   'Ae.Aegypti_1F_5D_LowMic': 0, \n",
    "                   'Ae.Albopictus_1F_14D_LowMic': 0,\n",
    "                   'Ae.Albopictus_1M_14D_LowMic': 0,\n",
    "                   'Ae.Aegypti_1M_5D_LowMic': 0}\n",
    "    con_count = {'Cx.Quin_1F_14D': 0 ,\n",
    "                  'Ae.Aegypti_1F_21D': 0,\n",
    "                  'An.Dirus_1F_5D': 0, \n",
    "                  'An.Dirus_1M_5D': 0,\n",
    "                  'An.Dirus_1M_14D': 0,\n",
    "                  'An.Minimus_1F_5D': 0,\n",
    "                  'An.Minimus_1M_5D': 0, \n",
    "                  'An.Dirus_1F_14D': 0,\n",
    "                  'Ae.Aegypti_1M_21D': 0,\n",
    "                  'Cx.Quin_1M_6D': 0, \n",
    "                  'Ae.Albopictus_1F_7D': 0,\n",
    "                  'Ae.Albopictus_1M_7D':0,\n",
    "                  'Cx.Quin_1F_6D': 0,\n",
    "                  'Cx.Quin_1M_14D': 0,\n",
    "                  'Ae.Aegypti_1F_5D': 0 ,\n",
    "                  'Ae.Albopictus_1F_14D': 0,\n",
    "                  'Ae.Albopictus_1M_14D': 0 ,\n",
    "                  'Ae.Aegypti_1M_5D': 0}\n",
    "    \n",
    "    X_train = []\n",
    "    dataY_train = []\n",
    "    X_test = []\n",
    "    dataY_test = []\n",
    "    X_val = []\n",
    "    dataY_val = []\n",
    "    count = 0\n",
    "    train_count = 0\n",
    "    val_count = 0\n",
    "    test_count = 0 \n",
    "    sr = 8000\n",
    "    target = 2400  # 0.3s\n",
    "    # loop files\n",
    "    for p in sorted_audio_files_name:\n",
    "        path = dir + p\n",
    "        row = p.split('_')\n",
    "        if path.endswith('.wav'):\n",
    "#             print('Currently working on ' + p)\n",
    "            sex_without_number = row[1][1]\n",
    "            sex_with_number = row[1]\n",
    "            sp_gender = row[0] + '_' + sex_without_number\n",
    "            x, sr = librosa.load(path, sr=sr)\n",
    "            # cut file\n",
    "            for i in range(0, len(x), int(target / 2)):  #overlap .15s\n",
    "                np.random.seed(i)\n",
    "                shift = np.random.randint(120)\n",
    "                y = x[i + shift : i + target + shift]\n",
    "                if (len(y) == target) and row[0] != \"An.Minimus\":\n",
    "                    key = recreate_mosquito_filename_without_noise(row[0], row[1], row[2], row[5])\n",
    "                    current_frame = count_species[key]\n",
    "                    count_species[key] = count_species[key] + 1\n",
    "                    count += 1\n",
    "                    # validation\n",
    "                    if (current_frame >= (fold_round * math.floor(total_data[key] * 0.1))) and (current_frame < ((fold_round + 1) * math.floor(total_data[key] * 0.1))):\n",
    "                        dataY_val.append(sp_gender)\n",
    "                        X_val.append(y)\n",
    "                        val_count += 1\n",
    "#                     test\n",
    "                    elif (current_frame >= ((test_index + 1) * math.floor(total_data[key] * 0.1))) and (current_frame < ((test_index + 2) * math.floor(total_data[key] * 0.1))):\n",
    "                        if (row[5] == \"LowMic\") and (low_count[key] < count_test_low[key]):\n",
    "                            low_count[key] += 1\n",
    "                            dataY_test.append(sp_gender)    \n",
    "                            X_test.append(y)\n",
    "                            test_count += 1\n",
    "                    else:\n",
    "                        dataY_train.append(sp_gender)\n",
    "                        X_train.append(y)\n",
    "                        train_count += 1\n",
    "    return X_train, X_val, X_test, dataY_train, dataY_val, dataY_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OverlayAndDelayPerBatchTestSet (np_mosquito, sr, f, seed, is_noise_cancelled):\n",
    "    '''  \n",
    "          Microphone 1 (M1) is on the left side.\n",
    "          Microphone 2 (M2) is on the right side.\n",
    "          Parameters\n",
    "          x is position on x-axis (meter).\n",
    "          y is position on y-axis (meter).\n",
    "          length is the distance between microphone 1 and microphone 2.\n",
    "          coordinate (0,0) is at the middle between 2 microphones.\n",
    "          f is scaling factor to scale the level of mosquito sound.\n",
    "    '''\n",
    "    length = 0.3\n",
    "    delta_m = 0.5\n",
    "    x = ((length * delta_m)) \n",
    "    y = 0.05\n",
    "    M1 = (-(length/2) , 0)\n",
    "    M2 = (length/2 , 0)\n",
    "#     print(M1, M2)\n",
    "\n",
    "    xM1_dist, xM2_dist, delay_xM1, delay_xM2 = calculate_delay(M1, M2, sr, x , y)\n",
    "#         *********  random the different source of environmental noise *********\n",
    "\n",
    "    random.seed(seed)\n",
    "    randomIndex = random.randint(0,len(noise_sound) - 1)\n",
    "    mono_M1, mono_M2 = separate_noise_channels(noise_sound[randomIndex])\n",
    "\n",
    "\n",
    "#         ********* multiply f and 1/ distance from inverse distance law  *********\n",
    "    random_f = f\n",
    "    f_M1 = MultiplyFactorToSound(np_mosquito, random_f/xM1_dist)\n",
    "    f_M2 = MultiplyFactorToSound(np_mosquito, random_f/xM2_dist)\n",
    "\n",
    "\n",
    "#         ********* add delay to mosquito sound *********\n",
    "    delay_xM1_np = np.tile(delay_xM1, (f_M1.shape[0],1))\n",
    "    delay_xM2_np = np.tile(delay_xM2, (f_M2.shape[0],1))\n",
    "    delayed_mosquito_M1 = np.concatenate((delay_xM1_np,f_M1), axis=1)\n",
    "    delayed_mosquito_M2 = np.concatenate((delay_xM2_np,f_M2), axis=1)\n",
    "    delayed_mosquito_M1 = np.delete(delayed_mosquito_M1,np.s_[2400:delayed_mosquito_M1.shape[1]],1)    \n",
    "    delayed_mosquito_M2 = np.delete(delayed_mosquito_M2,np.s_[2400:delayed_mosquito_M2.shape[1]],1) \n",
    "    delayed_cancelled_mosquito_sound = np.subtract(delayed_mosquito_M2, delayed_mosquito_M1)\n",
    "\n",
    "\n",
    "#         ********* random shifted noise array  *********\n",
    "    noise_1, noise_2 = random_shifted_noise_with_seed(mono_M1,mono_M2, 2400, seed)\n",
    "    overlay_M1 = np.add(delayed_mosquito_M1, noise_1.reshape(1,delayed_mosquito_M1.shape[1]))\n",
    "    overlay_M2 = np.add(delayed_mosquito_M2, noise_2.reshape(1,delayed_mosquito_M2.shape[1]))\n",
    "    denoised_sound = np.subtract(overlay_M2, overlay_M1)\n",
    "    if is_noise_cancelled:\n",
    "        sample_list = [delayed_cancelled_mosquito_sound, denoised_sound]\n",
    "    elif is_noise_cancelled == False:\n",
    "        sample_list = [delayed_mosquito_M2, overlay_M2]\n",
    "    return random.choice(sample_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DelayTestSet (np_mosquito, sr, f, is_noise_cancelled):\n",
    "    '''  \n",
    "          Microphone 1 (M1) is on the left side.\n",
    "          Microphone 2 (M2) is on the right side.\n",
    "          Parameters\n",
    "          x is position on x-axis (meter).\n",
    "          y is position on y-axis (meter).\n",
    "          length is the distance between microphone 1 and microphone 2.\n",
    "          coordinate (0,0) is at the middle between 2 microphones.\n",
    "          f is scaling factor to scale the level of mosquito sound.\n",
    "          offset is where the mosquito sound start on the noise recording  (second)\n",
    "    '''\n",
    "    length = 0.3\n",
    "    delta_m = 0.5\n",
    "    x = ((length * delta_m)) \n",
    "    y = 0.05\n",
    "    M1 = (-(length/2) , 0)\n",
    "    M2 = (length/2 , 0)\n",
    "    xM1_dist, xM2_dist, delay_xM1, delay_xM2 = calculate_delay(M1, M2, sr, x , y)\n",
    "\n",
    "#         ********* mutiply f factor and inverse distance *********\n",
    "    f_M1 = MultiplyFactorToSound(np_mosquito, f/xM1_dist)\n",
    "    f_M2 = MultiplyFactorToSound(np_mosquito, f/xM2_dist)\n",
    "\n",
    "#         ********* add delay to mosquito sound *********\n",
    "    delay_xM1_np = np.tile(delay_xM1, (f_M1.shape[0],1))\n",
    "    delay_xM2_np = np.tile(delay_xM2, (f_M2.shape[0],1))\n",
    "    delayed_mosquito_M1 = np.concatenate((delay_xM1_np,f_M1), axis=1)\n",
    "    delayed_mosquito_M2 = np.concatenate((delay_xM2_np,f_M2), axis=1)\n",
    "    delayed_mosquito_M1 = np.delete(delayed_mosquito_M1,np.s_[2400:delayed_mosquito_M1.shape[1]],1)    \n",
    "    delayed_mosquito_M2 = np.delete(delayed_mosquito_M2,np.s_[2400:delayed_mosquito_M2.shape[1]],1) \n",
    "    denoised_sound = np.subtract(delayed_mosquito_M2, delayed_mosquito_M1)\n",
    "    if is_noise_cancelled:\n",
    "        return denoised_sound\n",
    "    elif is_noise_cancelled == False:\n",
    "        return delayed_mosquito_M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_or_test_set(dataX, seed_increment, is_noise_cancelled):\n",
    "    f1 = []\n",
    "    f15 = []\n",
    "    f2 = []\n",
    "    for idx, val in enumerate(dataX):\n",
    "    #     from shape (xxxx) -> (1, xxxx)\n",
    "        seed = seed_increment + idx\n",
    "        f1.append(OverlayAndDelayPerBatchTestSet(np.array(dataX[idx]).reshape(1, np.array(dataX[idx]).shape[0]), 8000,1, seed, is_noise_cancelled))    \n",
    "        f15.append(OverlayAndDelayPerBatchTestSet(np.array(dataX[idx]).reshape(1, np.array(dataX[idx]).shape[0]), 8000,1.5,seed, is_noise_cancelled))    \n",
    "        f2.append(OverlayAndDelayPerBatchTestSet(np.array(dataX[idx]).reshape(1, np.array(dataX[idx]).shape[0]), 8000,2,seed, is_noise_cancelled))    \n",
    "    # totalX_test\n",
    "    totalX_test = np.concatenate((f1, f15, f2))\n",
    "    np.array(totalX_test).shape\n",
    "    # reshape the f1,f15,f2 test set\n",
    "    f1 = np.array(f1)\n",
    "    f15 = np.array(f15)\n",
    "    f2 = np.array(f2)\n",
    "    f1 = np.array(f1).reshape(f1.shape[0], f1.shape[2], f1.shape[1])\n",
    "    f15 = np.array(f15).reshape(f15.shape[0], f15.shape[2], f15.shape[1])\n",
    "    f2 = np.array(f2).reshape(f2.shape[0], f2.shape[2], f2.shape[1])\n",
    "    totalX_test = np.array(totalX_test).reshape(totalX_test.shape[0], totalX_test.shape[2], totalX_test.shape[1])\n",
    "    return f1, f15, f2, totalX_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_set_without_noise(X_test, seed_increment, is_noise_cancelled):\n",
    "    f1_no_noise = []\n",
    "    f15_no_noise = []\n",
    "    f2_no_noise = []\n",
    "    for idx, val in enumerate(X_test):\n",
    "    #     from shape (xxxx) -> (1, xxxx)\n",
    "        val = np.array(val)\n",
    "        f1_no_noise.append(DelayTestSet(np.array(val).reshape(1, np.array(val).shape[0]), 8000,1, is_noise_cancelled))    \n",
    "        f15_no_noise.append(DelayTestSet(np.array(val).reshape(1, np.array(val).shape[0]), 8000,1.5, is_noise_cancelled))    \n",
    "        f2_no_noise.append(DelayTestSet(np.array(val).reshape(1, np.array(val).shape[0]), 8000,2, is_noise_cancelled))    \n",
    "    # totalX_test\n",
    "    totalX_test_no_noise = np.concatenate((f1_no_noise, f15_no_noise, f2_no_noise))\n",
    "    np.array(totalX_test_no_noise).shape\n",
    "    # reshape the f1,f15,f2, test set\n",
    "    f1_no_noise = np.array(f1_no_noise)\n",
    "    f15_no_noise = np.array(f15_no_noise)\n",
    "    f2_no_noise = np.array(f2_no_noise)\n",
    "\n",
    "    f1_no_noise = np.array(f1_no_noise).reshape(f1_no_noise.shape[0], f1_no_noise.shape[2], f1_no_noise.shape[1])\n",
    "    f15_no_noise = np.array(f15_no_noise).reshape(f15_no_noise.shape[0], f15_no_noise.shape[2], f15_no_noise.shape[1])\n",
    "    f2_no_noise = np.array(f2_no_noise).reshape(f2_no_noise.shape[0], f2_no_noise.shape[2], f2_no_noise.shape[1])\n",
    "    totalX_test_no_noise = np.array(totalX_test_no_noise).reshape(totalX_test_no_noise.shape[0], totalX_test_no_noise.shape[2], totalX_test_no_noise.shape[1])\n",
    "    return f1_no_noise, f15_no_noise,f2_no_noise, totalX_test_no_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report(model, X_test, y_test):\n",
    "    time.sleep(4)\n",
    "    prediction = np.argmax(model.predict(X_test), axis = 1)\n",
    "    df = pd.DataFrame(classification_report(np.argmax(y_test, axis = 1), prediction, output_dict=True))\n",
    "    inv_label_dict = { str(value) : key for (key, value) in label_dict.items() }\n",
    "    df.rename(columns=inv_label_dict, inplace=True)\n",
    "    df = df.round(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_result(model, X_test, y_test, row_index):\n",
    "    _, score = model.evaluate(X_test, y_test, batch_size=32, verbose=0)\n",
    "    res = get_report(model, X_test, y_test)\n",
    "    return res.iloc[row_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_model(saved_model_dir, training_round):\n",
    "    formatted_training_round = \"_round\" + str(training_round + 1)\n",
    "    for model_name in os.listdir(saved_model_dir):\n",
    "        if formatted_training_round in model_name:\n",
    "            return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_file(classification_result, metric_name, model_directory):\n",
    "    classification_result.to_csv(f\"{model_directory}{metric_name}_classification_result_raw.csv\", index = False)\n",
    "    average_res = pd.DataFrame(classification_result.groupby(\"Test dataset\").mean())\n",
    "    average_res = average_res.drop(columns=['round'])\n",
    "    average_res.to_csv(f\"{model_directory}{metric_name}_classification_result_avg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data/mosquito_recordings/\"\n",
    "model_dir = \"./models/\"\n",
    "export_result_dir = \"./classification_results/\"\n",
    "sorted_audio_files_name = load_data(data_dir)\n",
    "data_count  = count_data(sorted_audio_files_name, data_dir)\n",
    "total_res_f1_score = pd.DataFrame()\n",
    "total_res_precision = pd.DataFrame()\n",
    "total_res_recall = pd.DataFrame()\n",
    "\n",
    "for fold_round in range(10):\n",
    "    print(f\" ********** round {fold_round} ********** \")\n",
    "    X_train, X_val, X_test, dataY_train, dataY_val, dataY_test = split_data(sorted_audio_files_name, data_count,  fold_round, data_dir)\n",
    "#     create test set with noise cancellation \n",
    "    f1,f15,f2, totalX_test = create_validation_or_test_set(X_test, 0, is_noise_cancelled = True)\n",
    "    f1_no_noise,f15_no_noise, f2_no_noise, totalX_test_no_noise =  create_test_set_without_noise(X_test, 0,is_noise_cancelled = True)\n",
    "    _,_,_, totalX_val = create_validation_or_test_set(X_val, 2500, is_noise_cancelled = True)\n",
    "#     encode data\n",
    "    print(\" ------ Encode data ------\")\n",
    "    y_train, label_dict = encode_data(dataY_train)\n",
    "    y_test, label_dict = encode_data(dataY_test)\n",
    "    y_val,label_dict = encode_data(dataY_val)\n",
    "#     balance data\n",
    "    print(\"------ Balance data ------\")\n",
    "    X_train, y_train = balance_train(X_train, y_train)\n",
    "#     reshape data\n",
    "    print(\"------ Reshape data ------\")\n",
    "    X_train, totalX_val, X_test, y_train, y_val, y_test = reshape_data(X_train, totalX_val, X_test, y_train, y_val, y_test) \n",
    "    y_test_2x = np.concatenate((y_test, y_test))\n",
    "    y_test_6x = np.concatenate((y_test, y_test, y_test, y_test, y_test, y_test))\n",
    "#     evaluate data\n",
    "    totalX_test_data_de = [np.concatenate((f1, f1_no_noise)), np.concatenate((f15, f15_no_noise)), \n",
    "                           np.concatenate((f2, f2_no_noise)), np.concatenate((totalX_test, totalX_test_no_noise))]\n",
    "\n",
    "    totalY_test_data = [y_test_2x,y_test_2x,y_test_2x,y_test_6x]\n",
    "\n",
    "    row_label = [\"Denoised Wingbeats (F=1) + Denoised Clean Wingbeats (F=1) \",\n",
    "        \"Denoised Wingbeats (F=1.5) + Denoised Clean Wingbeats (F=1.5)\",\n",
    "        \"Denoised Wingbeats (F=2) + Denoised Clean Wingbeats (F=2)\",\n",
    "        \"Denoised Wingbeats (F=1,1.5,2) + Denoised Clean Wingbeats (F=1,1.5,2)\"]\n",
    "\n",
    "    saved_model_cp_dir = f\"{model_dir}\"\n",
    "    model_filename = find_optimal_model(model_dir, fold_round)\n",
    "    print(f\"----- ROUND: {fold_round + 1}, MODEL: {model_filename}\")\n",
    "    min_val_model = tf.keras.models.load_model(model_dir + model_filename)\n",
    "    for idx, test_data in enumerate(totalX_test_data_de):\n",
    "        for metric_index in range(3):                    \n",
    "            res = get_test_result(min_val_model, test_data, totalY_test_data[idx], metric_index)\n",
    "            res = pd.DataFrame(res).transpose()\n",
    "\n",
    "            res.insert(0, 'round', fold_round + 1)\n",
    "            res.insert(0, 'Test dataset', row_label[idx])\n",
    "            if metric_index == 0:\n",
    "                total_res_precision = total_res_precision.append(res)\n",
    "            elif metric_index == 1:\n",
    "                total_res_recall = total_res_recall.append(res)\n",
    "            elif metric_index == 2:\n",
    "                total_res_f1_score = total_res_f1_score.append(res)\n",
    "\n",
    "write_csv_file(total_res_precision, \"precision\",  export_result_dir)\n",
    "write_csv_file(total_res_recall, \"recall\", export_result_dir)\n",
    "write_csv_file(total_res_f1_score, \"f1_score\", export_result_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNqZymX4i5UH1ncLYryJKfY",
   "collapsed_sections": [],
   "mount_file_id": "1ijagVMjp2fL7dcs2sp3ifMYYEz3VUtIp",
   "name": "Copy of DNN.ipynb",
   "provenance": [
    {
     "file_id": "1BH0XatyC2IGbXhy0v3ipt3lyB8hekj56",
     "timestamp": 1610184810784
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
